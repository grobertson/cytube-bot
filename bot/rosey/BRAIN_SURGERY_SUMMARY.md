# LLM Integration - Brain Surgery Sprint Summary

## What We Built

### Core Infrastructure âœ…
- **Provider Abstraction** - `LLMProvider` base class supporting multiple backends
- **Ollama Provider** - Local/remote Ollama inference with network support
- **OpenRouter Provider** - Cloud API access to Claude, GPT-4, etc.
- **LLMClient** - High-level client with per-user conversation context

### Sophisticated Trigger System âœ…

- **Direct Mentions** - Responds when bot's username mentioned (e.g., "CynthiaRothbot what's playing?")
- **Commands** - `!ai`, `!ask` prefix commands
- **Keyword Triggers** - Configurable phrases with probability and cooldown
  - Example: "toddy" â†’ 10% chance, 5min cooldown
  - Example: "boobs" â†’ 5% chance, 10min cooldown
- **Ambient Chat** - Participates every N messages with randomness
- **User Greetings** - Welcome users on join with:
  - Per-user probabilities
  - Idle threshold (only greet if away >60min)
  - Moderator-only option
  - Status change greetings

### Configuration Flexibility âœ…
- **log_only Mode** - Test responses without spamming chat
- **Individual Enable/Disable** - Every trigger can be toggled
- **Probability Controls** - Fine-tune response frequency (0.0-1.0)
- **Cooldowns** - Prevent spam on keyword triggers
- **Randomness** - Ambient chat timing variance

### Integration with Rosey âœ…

- **Event Handlers** - chatMsg, pm, addUser
- **Username Detection** - Bot responds to its CyTube username (e.g., "CynthiaRothbot", "SaveTheRobots")
- **Prompt Extraction** - Automatically removes commands/mentions
- **Context Management** - Per-user conversation history
- **Comprehensive Logging** - Trigger reasons, responses, errors
- **Graceful Degradation** - Works without LLM if not configured
- **Multi-Instance Ready** - Multiple bot instances can coexist in same channel

## Configuration Example

```json
{
  "llm": {
    "enabled": true,
    "provider": "ollama",
    "log_only": false,
    "ollama": {
      "base_url": "http://192.168.1.100:11434",
      "model": "llama3"
    },
    "triggers": {
      "enabled": true,
      "direct_mention": true,
      "commands": ["!ai", "!ask"],
      "ambient_chat": {
        "enabled": true,
        "every_n_messages": 20,
        "randomness": 0.5
      },
      "keywords": [
        {
          "phrases": ["toddy", "the toddy"],
          "probability": 0.1,
          "cooldown_seconds": 300
        }
      ],
      "greetings": {
        "enabled": true,
        "on_join": {
          "enabled": true,
          "probability": 0.2,
          "specific_users": {
            "alice": 1.0
          }
        }
      }
    }
  }
}
```

## Usage Examples

### Direct Interaction

```text
User: CynthiaRothbot what's playing?
CynthiaRothbot: [LLM response about current video]

User: hey SaveTheRobots tell me a joke
SaveTheRobots: [LLM generated joke]

User: !ai what's the weather like?
Bot: [LLM response]
```

### Keyword Triggers

```text
User: Praise toddy!
[10% chance bot responds with toddy worship]

User: talking about boobs
[5% chance bot makes a tasteful comment]
```

### Ambient Participation

```text
[After ~20 messages of chat]
Bot: [Relevant comment about recent conversation]
```

### User Greetings

```text
Alice joins (always greeted)
CynthiaRothbot: Welcome back Alice!

Bob joins (20% chance)
[Maybe greeted, maybe not]
```

## Testing

### Test LLM Setup
```bash
cd bot/rosey
python test_llm.py
```

### Test with log_only
Set `"log_only": true` in config to see what Rosey would say without actually saying it:
```
[2025-11-10 15:30:45] [INFO] Trigger: keyword:toddy | User: alice | Message: praise toddy
[2025-11-10 15:30:46] [INFO] [LOG ONLY] Would respond: All hail toddy, the alpha and omega!
```

## Next Steps

### Planned Enhancements
1. **PM Integration** - Private AI conversations
2. **Sentiment Analysis** - Respond based on chat mood
3. **Tool Use** - Let LLM control playlist, kick users, etc.
4. **Context Enrichment** - Include channel state in prompts
5. **Personality Modes** - Different personalities for different contexts
6. **Rate Limiting** - Per-user request limits
7. **Cost Tracking** - Monitor OpenRouter usage
8. **A/B Testing** - Compare different trigger configurations

### Immediate Todos
- [ ] Test with real Ollama server
- [ ] Refine prompt.md personality
- [ ] Tune trigger probabilities based on user feedback
- [ ] Add more keyword phrases
- [ ] Implement PM handler
- [ ] Add status change greeting logic

## Architecture

```
rosey.py
â”œâ”€â”€ LLMHandlers (event handlers)
â”‚   â”œâ”€â”€ handle_chat_message()
â”‚   â”œâ”€â”€ handle_pm()  [stub]
â”‚   â””â”€â”€ handle_user_join()
â”‚
llm/
â”œâ”€â”€ client.py (LLMClient - conversation management)
â”œâ”€â”€ providers.py (OllamaProvider, OpenRouterProvider)
â””â”€â”€ triggers.py (TriggerConfig, TriggerManager)
```

## Flexibility Highlights

1. **Granular Control** - Every aspect is configurable
2. **Probabilistic Responses** - Feels natural, not bot-like
3. **Cooldown System** - Prevents joke fatigue
4. **User-Specific Behavior** - Treat mods/friends differently
5. **Randomness** - Ambient chat feels organic
6. **Log-Only Mode** - Safe testing
7. **Multiple Providers** - Switch between local/remote easily

## Commit History

1. `993312a` - LLM foundation (providers, client)
2. `e1b9267` - Remote Ollama documentation
3. `63f0e31` - Trigger system and Rosey integration

---

**Brain surgery complete! ðŸ§ âš¡**
